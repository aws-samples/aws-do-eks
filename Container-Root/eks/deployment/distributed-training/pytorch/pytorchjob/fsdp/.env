#!/bin/bash

# Configuration for FSDP jobs

## AWS
export AWS_REGION=us-west-2
export ACCOUNT=$(aws sts get-caller-identity --query Account --output text)

## Docker Image
export REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/
export IMAGE=fsdp2
## DOCKERFILE_EXT=nanogpt-sockets|nanogpt-efa|nanogpt-efa-prebuilt|llama2-sockets|llama2-efa|llama2-efa-dlc
export DOCKERFILE_EXT=nanogpt-efa-prebuilt
export TAG=":${DOCKERFILE_EXT}"

## FSDP Job
export JOB_NAME=fsdp2
export RDZV_HOST=etcd
export RDZV_PORT=2379
## NUM_WORKERS - Default 2, set to number of worker nodes
export NUM_WORKERS=2
## EFA_PER_WORKER - Default 0, number of EFA adapters per node. For G4dn.metal this is 1, for P4 use 4, for P5 use 32
export EFA_PER_WORKER=1
## GPU_PER_WORKER - number of GPUs per worker, the number of GPUs for the selected instance type.
export GPU_PER_WORKER=1
## INSTANCE_TYPE=p4de.24xlarge|p5.48xlarge|g4dn.metal|g4dn.8xlarge(default)|etc
export INSTANCE_TYPE=ml.g5.8xlarge
## FI_PROVIDER=sockets(default)|efa
export FI_PROVIDER=efa

## Model
## Support is available for NanoGPT and Llama2. Only one of the sections below should be uncommented and should match the DOCKERFILE_EXT section above

## NanoGPT
## NanoGPT fsdp2 train command
# Ref: https://github.com/lessw2020/nanoGPT_2d.git
## DATASET_PREP - path to the dataset perparation script from the nanoGPT_2d repo
export DATASET_PREP="data/shakespeare_char/prepare.py"
#export DATASET_PREP="data/shakespeare/prepare.py"
## ENABLE_CHECKPOINT=False(default)|True - enable saving of checkpoint at end of each epoch
export ENABLE_CHECKPOINT=False
## MODEL_CONFIG - path to model configuration in nanoGPT_2d repo
#export MODEL_CONFIG="config/eval_gpt2.py"
export MODEL_CONFIG="config/train_shakespeare_char.py"
#export MODEL_CONFIG="config/finetune_shakespeare.py"
#export NUM_ITERATIONS=500
#export CMD="sed -e \\\"s/500/${NUM_ITERATIONS}/g\\\" -i ${MODEL_CONFIG} && python ${DATASET_PREP} && python -m torch.distributed.run --nproc-per-node=$GPU_PER_WORKER train.py --enable_save_checkpoint=${ENABLE_CHECKPOINT} ${MODEL_CONFIG}"
export CMD="python ${DATASET_PREP} && python -m torch.distributed.run --nproc-per-node=$GPU_PER_WORKER train.py --enable_save_checkpoint=${ENABLE_CHECKPOINT} ${MODEL_CONFIG}"

## Llama2
## Register at Huggingface and get a token by visiting: https://huggingface.co/docs/hub/security-tokens, then insert your token here
#export HF_TOKEN="<insert_your_huggingface_token_here>"
## Llama2 MODEL_NAME=meta-llama/Llama-2-7b-hf | meta-llama/Llama-2-13b-hf | meta-llama/Llama-2-70b-hf
#export MODEL_NAME=meta-llama/Llama-2-7b-hf
## Llama2 train command
### samsum dataset
#export CMD="huggingface-cli login --token ${HF_TOKEN} && torchrun --nproc_per_node=${GPU_PER_WORKER} --nnodes=${NUM_WORKERS} recipes/finetuning/finetuning.py --num_epochs=3 --batch_size_training=3 --enable_fsdp --model_name $MODEL_NAME --output_dir ."
### arrow dataset 
#export CMD="huggingface-cli login --token ${HF_TOKEN} && torchrun --nproc_per_node=${GPU_PER_WORKER} --nnodes=${NUM_WORKERS} recipes/finetuning/finetuning.py --num_epochs=3 --batch_size_training=16 --enable_fsdp --low_cpu_fsdp --batching_strategy padding --model_name $MODEL_NAME --dataset 'custom_dataset' --custom_dataset.file 'examples/custom_dataset.py' --output_dir ."
